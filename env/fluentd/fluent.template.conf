# In v1 configuration, type and id are @ prefix parameters.
# @type and @id are recommended. type and id are still available for backward compatibility

## built-in TCP input
## ${DOLLAR} echo <json> | fluent-cat <tag>
# <source>
#   @type forward
#   @id forward_input
# </source>

## built-in UNIX socket input
#<source>
#  @type unix
#</source>

# HTTP input
# http://localhost:8888/<tag>?json=<json>
# <source>
#   @type http
#   @id http_input
#
#   port 8888
# </source>

## File input
## read apache logs with tag=apache.access
#<source>
#  @type tail
#  format apache
#  path /var/log/httpd-access.log
#  tag apache.access
#</source>
# <source>
#   @type tail
#   path /input/log.json
#   tag test.*
#   <parse>
#     @type json
#   </parse>
# </source>

## Mutating event filter
## Add hostname and tag fields to apache.access tag events
#<filter apache.access>
#  @type record_transformer
#  <record>
#    hostname ${DOLLAR}{hostname}
#    tag ${DOLLAR}{tag}
#  </record>
#</filter>

## Selecting event filter
## Remove unnecessary events from apache prefixed tag events
#<filter apache.**>
#  @type grep
#  include1 method GET # pass only GET in 'method' field
#  exclude1 message debug # remove debug event
#</filter>

# Listen HTTP for monitoring
# http://localhost:24220/api/plugins
# http://localhost:24220/api/plugins?type=TYPE
# http://localhost:24220/api/plugins?tag=MYTAG
# <source>
#   @type monitor_agent
#   @id monitor_agent_input
#
#   port 24220
# </source>

# Listen DRb for debug
# <source>
#   @type debug_agent
#   @id debug_agent_input
#
#   bind 127.0.0.1
#   port 24230
# </source>

## match tag=apache.access and write to file
#<match apache.access>
#  @type file
#  path /var/log/fluent/access
#</match>

## match tag=debug.** and dump to console
# <match debug.**>
#   @type stdout
#   @id stdout_output
# </match>

# match tag=system.** and forward to another fluent server
# <match system.**>
#   @type forward
#   @id forward_output
#
#   <server>
#     host 192.168.0.11
#   </server>
#   <secondary>
#     <server>
#       host 192.168.0.12
#     </server>
#   </secondary>
# </match>

## match tag=myapp.** and forward and write to file
#<match myapp.**>
#  @type copy
#  <store>
#    @type forward
#    buffer_type file
#    buffer_path /var/log/fluent/myapp-forward
#    retry_limit 50
#    flush_interval 10s
#    <server>
#      host 192.168.0.13
#    </server>
#  </store>
#  <store>
#    @type file
#    path /var/log/fluent/myapp
#  </store>
#</match>

## match fluent's internal events
#<match fluent.**>
#  @type null
#</match>

## match not matched logs and write to file
#<match **>
#  @type file
#  path /var/log/fluent/else
#  compress gz
#</match>

# <match test.**>
#   @type copy
#   @id test_output
#
#   <store>
#       @type file
#       path /output_file/debug
#       format json
#       time_slice_format %Y%m%d
#       time_slice_wait 10m
#       time_format %Y%m%dT%H%M%S%z
#       # compress gzip
#       buffer_type memory
#       flush_interval 1s
#       utc
#   </store>
#
#     # <store>
#     #   @type stdout
#     #   @id stdout_output2
#     # </store>
#
#     # <store>
#     #   @type elasticsearch
#     #   host store
#     #   port 9200
#     #   logstash_format true
#     # </store>
# </match>

## Label: For handling complex event routing
#<label @STAGING>
#  <match system.**>
#    @type forward
#    @id staging_forward_output
#    <server>
#      host 192.168.0.101
#    </server>
#  </match>
#</label>

# Twitter
# <source>
#   @type twitter
#   consumer_key        ${TWITTER_CONSUMER_KEY}
#   consumer_secret     ${TWITTER_CONSUMER_SECRET}
#   access_token        ${TWITTER_ACCESS_TOKEN}
#   access_token_secret ${TWITTER_ACCESS_TOKEN_SECRET}
#   tag                 input.twitter.sampling   # Required
#   timeline            sampling                 # Required (tracking or sampling or location or userstream)
#   keyword             'builderscon'   # Optional (keyword has priority than follow_ids)
# </source>
#
# <match input.twitter.sampling>
#   @type copy
#     <store>
#       @type stdout
#     </store>
#
#   <store>
#       @type file
#       path /output_file/debug
#       format json
#   </store>
#
#     # <store>
#     #   @type elasticsearch
#     #   host store
#     #   port 9200
#     #   #logstash_format true
#     #   index_name fluentd.${tag}
#     # </store>
# </match>

# Google Analytics
# <source>
#   @type googleanalytics
#   tag analytics
#   id 'ga:196861108'
#   start_date yesterday
#   end_date today
#   dimensions 'ga:pagePath'
#   metrics 'ga:hits'
# </source>
#
#
# <match analytics>
#   @type copy
#   <store>
#     @type stdout
#   </store>
# </match>

# <source>
#   @type googleanalytics
#   tag analytics
#   id 'ga:196861108'
#   start_date yesterday
#   end_date today
#   dimensions 'ga:pagePath'
#   metrics 'ga:hits'
# </source>

<source>
  @type mysql_replicator

  # Set connection settings for replicate source.
  host mysql
  username root
  password password
  database xxx_service

  # Set replicate query configuration.
  query SELECT id, name from products;
  primary_key id
  interval 10s

  # Enable detect deletion event not only insert/update events. (default: yes)
  # It is useful to use `enable_delete no` that keep following recently updated record with this query.
  # `SELECT * FROM search_test WHERE DATE_ADD(updated_at, INTERVAL 5 MINUTE) > NOW();`
  enable_delete yes

  # Format output tag for each events. Placeholders usage as described below.
  tag replicator.myweb.search_test.${event}.${primary_key}
  # ${event} : the variation of row event type by insert/update/delete.
  # ${primary_key} : the value of `replicator_manager.settings.primary_key` in manager table.
</source>

<match replicator.**>
  @type mysql_replicator_elasticsearch

  # Set Elasticsearch connection.
  host elasticsearch
  port 9200

  # You can configure to use SSL for connecting to Elasticsearch.
  # ssl true

  # Basic authentication credentials can be configured
  # username basic_auth_username
  # password basic_auth_password

  # Set Elasticsearch index, type, and unique id (primary_key) from tag.
  tag_format (?<core_name>[^\.]+)\.(?<event>[^\.]+)\.(?<primary_key>[^\.]+)$

  # Set frequency of sending bulk request to Elasticsearch node.
  flush_interval 5s

  # Set maximum retry interval (required fluentd >= 0.10.41)
  max_retry_wait 1800

  # Queued chunks are flushed at shutdown process.
  # It's sample for td-agent. If you use Yamabiko, replace path from 'td-agent' to 'yamabiko'.
  flush_at_shutdown yes
  buffer_type file
  buffer_path /var/log/td-agent/buffer/mysql_replicator_elasticsearch
</match>